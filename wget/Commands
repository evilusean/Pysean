Needed to figure out how to get CSS from a site, 'inspect' chrome tools wasn't working, wget can pull an entire site for us to look at.
Ok, what seems to work = use below wget command, download site, inspect site for the element you want CSS from, 
search for either 'style' or the 'element' you are trying to reproduce and use those styles(their posted into tags, not a file)

wget -p -k http://www.example.com/ #Use wget in terminal, It'll make a clone of site frontend html, css, js, svg etc. But not in one file as asked. Rather, it'll recreate the whole folder structure

#to get all audio files on a website, like pulling all MP3's to make an Anki deck for a lesson
wget -r -l1 -H -nd -A mp3 -e robots=off http://example/url 
-r turns on recursion and downloads all links on page
-l1 goes only one level of links into the page(this is really important when using -r)
-H spans domains meaning it will download links to sites that don't have the same domain
-nd means put all the downloads in the current directory instead of making all the directories in the path
-A mp3 filters to only download links that are mp3s(this can be a comma separated list of different file formats to search for multiple types)
-e robots=off just means to ignore the robots.txt file which stops programs like wget from crashing the site

Use webhttrack:
sudo apt-get install webhttrack #will install webhttrack
webhttrack #will open it up in your brwoser, after installing 
#go through the tutorial for above, select your folder you want to download the file in, put the URL for downloaded website, and it should start the download, and download ALL files, including CSS
