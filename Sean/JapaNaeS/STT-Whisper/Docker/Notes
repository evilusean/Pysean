Past Sean, used to have to spin up a VM to then run WhisperAI, on Ubuntu, then I upgraded to Arch(Btw.), and I had some problems with the graphics card/Cuda
So what I want to do, is create a dockerfile that can run in the background, everytime I want to translate some Japanese speech(Locally - no internet, that's cheating) - 
Ideally it would be able to real time translaSeans and 'chunk' the audio in sections, then translate that, and save to a text file with timestamps - that's alot of moving parts 
worst case ontario, get it to take an audio file and translate that to a text document with timestamps would be the MVP
Also I want a hotkey that I can press like 'ctrl + [' to start recording and 'ctrl + ]' to stop recording and then translate everything within and print it out to a text file,
  do I want to save these files, or just have temporary ones? If I use temporary, would make the code way easier to write, less moving parts, if save, I would have records for future Sean - 
So if I have an interview/meeting, just start and stop recording when they are talking, and It will also double as a record of the convo, in case there is some Japanese I don't understand, can review later.
Since I previously had trouble with cuda, I would need to figure out how to give it access to the graphics card - model won't run without cuda, cuda won't run without a graphics card
I'm going to use AI to save time, as I'm currently studying math-precalc-EE-slovak-Japanese-coding-etc and I don't have weeks to sink into this, just spent weeks learning docker, the sooner I do this, the better
Probably use claude, as I've had really good success with it in the past, although OpenAI-Whisper is the model I will be putting in docker, would their proprietary AI be better at coding their own AI STT model? idk
Maybe I should start with the Coqui-AI-Model TTS - would be easier to work with, I've had that running pretty succesfully to create anki cards with speech in the past, already have half code written just need docker
On second thought, although it is nice, the Japanese STT model is way nicer to have, and get's me closer to being a samurai in animeland - although I can get citizenship by descent in slovakia-I can also do that later

Will need to create a prompt - mention dockerfile, all dependancies, openAI whisper, what model of python I need, and access to GPU
TO DO: 
-Create Prompt
-Create a 'Dockerfile'
  '.dockerignore' 
  '.gitignore' - don't want to upload a multi gig model to github accidently
-Make sure all dependancies are installed
-Ensure everything on host machine to allow docker container to access GPU/Cuda is installed - and all dependancies to record and share audios and code volumes
-Create a 'Commands' that will detail commands to launch and use the docker container/'docker-compose.yml'

1. Host Machine Requirements
First, install these on your host machine:
Docker
NVIDIA GPU Drivers
NVIDIA Container Toolkit (nvidia-docker2)
Install NVIDIA Container Toolkit:
# Add NVIDIA package repositories
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
# Install nvidia-docker2
sudo apt-get update
sudo apt-get install -y nvidia-docker2
# Restart Docker daemon
sudo systemctl restart docker

!ERROR! : Ran above command, and got this :
sudo: apt-key: command not found
tee: /etc/apt/sources.list.d/nvidia-docker.list: No such file or directory - Will fix later, my nvidia drivers work for steam/lutris, might work already
# Unsupported distribution!
# Check https://nvidia.github.io/nvidia-docker
zsh: command not found: #
sudo: apt-get: command not found
sudo: apt-get: command not found
zsh: command not found: #

2. Project Structure
whisper-jp/
├── src/                    # Your source code directory
├── recordings/             # Directory for audio recordings
├── output/                 # Directory for transcriptions
├── Dockerfile
├── .dockerignore
├── .gitignore
└── docker-compose.yml      # For easier container management

3. Dockerfile 
4. Docker Compose
version: '3.8'

services:
  whisper:
    build: .
    container_name: whisper-jp
    volumes:
      - ./src:/app/src
      - ./recordings:/app/recordings
      - ./output:/app/output
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

5. .dockerignore
6. .gitignore

2nd Prompt Results =======================================================================================================================================================================================
# I saved everything for the 'Dockerfile' '.gitignore' and '.dockerignore' from my previous prompt, the volumes and file structure was wrong, so trying again
/mnt/sdb4/Code/Pysean/Sean/JapaNaeS/STT-Whisper/Docker/
├── src/                    # Source code directory
│   ├── __init__.py
│   ├── main.py            # Main application code
│   ├── audio_handler.py   # Audio recording/processing
│   └── translator.py      # Translation logic
├── data/
│   ├── audio/            # Raw audio recordings
│   │   ├── temp/        # For real-time recording
│   │   └── saved/       # For saved recordings
│   └── output/          # Translated text files
│       ├── transcripts/ # Japanese transcripts
│       └── translations/ # English translations
├── Dockerfile
├── docker-compose.yml
├── requirements.txt
├── .dockerignore
└── .gitignore
#changed 'Dockerfile', added 'docker-compose.yml' 'requirements.txt'
#created directories with below commands :
cd /mnt/sdb4/Code/Pysean/Sean/JapaNaeS/STT-Whisper/Docker
mkdir -p src data/audio/{temp,saved} data/output/{transcripts,translations}
touch data/audio/temp/.gitkeep data/audio/saved/.gitkeep \
      data/output/transcripts/.gitkeep data/output/translations/.gitkeep

#Host Machine Requirements - Just tried, one by one, not a a single one of these commands works,:
# Install NVIDIA drivers if not already installed
sudo ubuntu-drivers autoinstall
# Install NVIDIA Container Toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update
sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker
#commands were for ubuntu, here are the new ones:
sudo pacman -S nvidia nvidia-utils
sudo pacman -S nvidia-container-toolkit #threw an error :
git clone https://aur.archlinux.org/nvidia-container-toolkit.git
cd nvidia-container-toolkit
makepkg -si
yay -S nvidia-container-toolkit
# All of the above commands for nvidia-container-toolkit errored out, pacman, yay, and git clone/makepkg :
error: failed retrieving file 'nvidia-container-toolkit-1.16.2-1-x86_64.pkg.tar.zst.sig' from ca.mirrors.cicku.me : The requested URL returned error: 404
warning: failed to retrieve some files

sudo pacman -Syu #something must be missing, doing a system update, it's been a few weeks I think
# Install nvidia-container-runtime
sudo pacman -S nvidia-container-runtime
# If that doesn't work, try these packages
sudo pacman -S nvidia-docker nvidia-container-toolkit
# If the above doesn't work, try installing from the AUR using these package names
yay -S nvidia-container-runtime-bin
# or
yay -S nvidia-container-toolkit-bin

4. Configure Docker to use NVIDIA runtime
#After successful installation, edit or create the Docker daemon configuration:
sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json <<EOF
{
    "runtimes": {
        "nvidia": {
            "path": "/usr/bin/nvidia-container-runtime",
            "runtimeArgs": []
        }
    },
    "default-runtime": "nvidia"
}
EOF

#Restart docker:
sudo systemctl restart docker
#Test Install:
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

#Build and Run Container :
docker-compose build
docker-compose up -d

!Error! docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi
docker: Error response from daemon: could not select device driver "" with capabilities: [[gpu]].

Docker isn't detecting GPU Drivers - I already put a day into this project, and spent last 2 hours attempting to debug, same error, 2 hours later, putting this down for now
I got too much other stuff on my plate atm to spend days debugging
https://hub.docker.com/r/nvidia/cuda
https://docs.docker.com/engine/containers/resource_constraints/#gpu
https://github.com/blakeblackshear/frigate/discussions/5536
https://github.com/NVIDIA/nvidia-docker/issues/1682
#Fixed the error, 'docker use context default' #I think it was this? Idk, I tried alot, and reinstalled, cleared cache, reinstalled again, ran out of space, reinstalled, rebuilt again, and it worked

Ok, I got everything working correctly, it is transcribing the audio, and I can do real time with the mic(my mic is trash, but it works, 3 second chunks, pretty cool) so I can do large files, and real time
I also had a problem where it kept reinstalling base model of 150mb each time I ran it, should probably attempt to fix that before I rebuild again

Prompt:
Now I want to expand the program, so I can run it from any terminal, I also want it to do youtube videos, with playlist support, (if I flag '-p' before the URL it will go through a playlist)
I want the bash script to default to '-jp', if I flag '-yt' I want it to default to a single video, only if I add the '-p' flag then do a playlist, 
when saving the audio from a playlist, save it as the video name and not the URL, use yt-dlp, because youtube-dl library has stopped working
I want to add 2 files for realtime translation, I want one for mic ('-m' flag) and one for taking audio from the host pc ('-l' for local or default) 
I want it to default from taking audio the host PC stream (not mic - it should capture whatever I am playing on the PC and translate it) so 'translasean -jp -rt' will default to that 
and 'translasean -jp -rt -m' will do the mic, I will rarely use this, so don't make it the default, obviously everywhere I have the '-jp' flag I want the same functionality for '-sk' -slovak flag
I also want it to create another transcribed audio with each english and japanese sentence combined, I created a file in the same directory as 'transcripts' and 'translated' as 'transcribed-translated' for new files
  - for this text file, you will need only one time stamp, I want both the hiragana and romaji transcribed for the japanese, and then followed by english for each sentence that is translated
  I also want you to add a flag following each transcription as either '-jp' or '-sk' for each language in the filename title
So I will need multiple files, I will list them and what they do, with their flags, please give them to me all at once sequentially, and update the bash script I've linked for context:
'main.py' #this is the default program similar to what I already have, 'translasean -jp' will wait to see a file in the temp folder, transcribe it, translate it, combine those 2 into a third file and move the audio
'realtime-stream.py' #this will take the audio stream from my host pc and translate it in realtime, I want the bash script flags to be 'translasean -jp -rt'
'realtime-mic.py' #this will take the audio from my mic and translate it in real time, I want the bash script flags to be 'translasean -jp -rt -m', also make sure all of these work for '-sk' slovak as well
'youtube-single.py' #will download the audio from a single youtube video, transcribe, translate, and combine it, then save the audio, 
  I want this to be the default flag for the bash script 'translasean -jp -yt <URL>' where URL is the youtube URL I copied without it being surrounded in ' quote marks
'youtube-playlist.py' #this will download an entire playlist one by one, transcribe, translate, and combine, then save the audio as well I want this to be the '-p' flag 'translasean -jp -yt -p <URL>'
Here is the context to the current 'main.py' 'realtime.py' and 'translasean.sh', please make all the changes, and give them to me as all the files at once with their names, 
Everytime I spin up the container currently it has to download a 150mb base model, is there any way you can make that persist in the container volume? here is the 'Dockerfile'
I want all audio files that are downloaded from youtube or saved from streaming to be mp3's - when saving the text don't forget to add the flags '-jp' or '-sk' at the end of filename
give me all 5 new '.py' files at once, as complete files, and the new bashscript I can use to access them from any terminal, and try to correct the 'Dockerfile' so that the base model persists inbetween uses

Ran out of tokens for cursor AI/Claude, I don't even have enough 4 food(again) and I can't move forward, and I don't have time to do it by hand. I'm going back to languages/math for now, will attempt this again l8r.
Treat others how they treated you. F = -F. Step 1) Escape. Remember all those times where she didn't buy food for 2 months? It got so bad so often, I had to start taking notes, the 'Grievance List' will outlive her.
Oh, figured it out, you only get 50 free uses of the 'premium models' which claude AI is one a month, I can use up to 200 'cursor small' queries a month, so I should be able to finsih this - maybe
claude is pretty good, but gonna try anyways. It's hard to worry about coding, when I'm wondering where my next meal is going to come from, thank god I live in geriatric gynocentric vaxcattle failed state of canada.

Ok, it works, sorta, need to figure out why realtime stream is so laggy, that is what I really want, a background translator for interviews/meetings, just so I can get context on unknown words
What does work though is if you add an mp3 to 'data/audio/temp' and then 'docker compose up' it will go through each mp3 in that file transcribe it, translate it, and combine it, then move the audio file
So I've got half usability, realtime translation on a terminal while I sit in a meeting would be ideal, instead of recording the audio, then reviewing it later, I lose the real time context
The mic I did on the first iteraSean worked, so it might be the streaming, idk, I've got like 50 tabs in chrome open, an editor, obsidian open, could just be that. 
