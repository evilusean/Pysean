# TO DO: Find a way to scrape all the pdf documents for posterity - I am aware internet archive exists, but things have gone missing before.
# It's crazy how we live in a total panopticon surviellance society but anything related to '"them"' always goes missing, inconvenient truths = malinformation? misinformation? disinformation?
# here is what I wanted to scrape : https://www.courtlistener.com/docket/66683865/government-of-the-united-states-virgin-islands-v-jpmorgan-chase-bank-na/?page=2
# unfortunately, I don't have time to go through hundreds of PDF's, but future Sean might want them, I am currently working on Step 1) escape canada, while being Starve-A-Sean'ed, Again.
# just thought of a funny joke, PDF file = P_D_ file, lol, lmao
# some anon already combined all the PDFiles, and uploaded them, so I don't need to click thousands of times : https://mega.nz/folder/A64ymBqI#dTuOhKdBgFdheja4it0OyA
# There's so much meat here, JPM and ukrainian girls, many names, we already have the black book, I personally, don't have time to go through thousands of PDFiles
# A good alternative would be getting OCR to read the PDFiles, and feed it to an AI somehow, and get a written summary of all the names and source documents - Future Sean problem.
# /pol/ pole anons work :
# There is over a gig of PDFiles, people are already sleuthing through them, but even if you find anything, who do you appeal to? '"They"' own the ju-dicial system 
# and the media, and the tech companies, and the politicians, and the banks, and the ..... etc. I can do this for days, but I have other things to do. Like escaping canada
# Just the data folder: https://mega.nz/file/gRQBEB6S#oxLC5mHZxs2w1VYovxARQ4oX-w2M0xaqYM7RU_I6pN8
# Script: https://mega.nz/file/IIAH0KoD#3cghkrIC4rgue8IsozglvIQh33WRp3tcohve9NAaKdc
# Folder with documents: https://mega.nz/file/JJhBSZZA#NOLhhjlfdBUdydDBbbha07mTX1gTSHMDx_eq52P_WpU
# Giant pdf: https://mega.nz/file/QBwRhDqa#zq-hwceeNm-5aCy7N762hxB_RY9zVaV4DCptYNPHODg
# ScRaper, Lol, Lmao. 
# Epstein 2006 Grand Jury notes :  >https://www.mypalmbeachclerk.com/home/showpublisheddocument/4194/638554423710170000
# Epstein 2019 Federal Charges :  >https://www.justice.gov/usao-sdny/press-release/file/1180481/dl
# Epstein’s Financial Holdings :  >https://vicourts.hosted.civiclive.com/common/pages/DisplayFile.aspx?itemId=16364025
# Ghislaine Maxwell deposition (2020) :  >https://www.courthousenews.com/wp-content/uploads/2020/10/Maxwell-deposition-2016.pdf
# Jeffrey Epstein deposition :  >https://www.sec.gov/oso/epstein-deposition-and-exhibits
# DOJ Case Files and notices : 
# Epstein/Maxwell Depositions : >https://d.newsweek.com/en/file/468909/jeffrey-epstein-documents-full.pdf 
# Epstein Court Documents (2016) :>https://ia600705.us.archive.org/21/items/epsteindocs/Epstein-Docs.pdf
# Epstein “Little Black Book” (unredacted) : >https://ia600705.us.archive.org/21/items/epsteindocs/Jeffrey_Epstein39s_Little_Black_Book_unredacted.pdf
# Epstein Flight Logs : >https://www.documentcloud.org/documents/21165424-epstein-flight-logs-released-in-usa-vs-maxwell/

import requests as rq
from bs4 import BeautifulSoup
import os

if not os.path.isdir('Data'):
    os.mkdir('Data')
folder='Data\\'

try:
    f=open('page_source.txt','r',encoding='utf-8')
except:
    print('Put the page source of the Court Listener site into a file called "page_source.txt" in the same folder as the program')
    quit()

page_source=f.read()
f.close()

soup=BeautifulSoup(page_source,'lxml')

# Get Rows

rows1=soup.find_all('div',{'class':'row odd'})
rows2=soup.find_all('div',{'class':'row even'})
rows=rows1+rows2
print(len(rows))

# Get Documents

pdflinks=set()
for row in rows:

    divs=row.find_all('div',recursive=False)

    count=0
    for div in divs:
        count+=1

        # Document number
        
        if count==1:
            try:
                document_number=int(div.text)
            except:
                pass

        # Document date
            
        if count==2:
            span=div.find('span')
            if span!=None:
                document_date=span.get('title')
            else:
                document_date=div.text

        # Document
        
        if count==3:
            description=div.find('p').text

            subfolder=folder+'Document nr '+str(document_number)
            if not os.path.isdir(subfolder):
                os.mkdir(subfolder)
            else:
                clean_date=str(document_date).strip().replace(',','').replace('.','')
                with open(subfolder+'\\'+clean_date+'.txt', 'w') as f:
                    f.write(str(description))
                break

            with open(subfolder+'\\description.txt', 'w') as f:
                f.write('Document date: '+str(document_date)+'\n\n\n'+str(description))
            
            docs=div.find_all('div',recursive=False)
            for doc in docs:
                parts=doc.find_all('div',recursive=False)
                relative_name=parts[0].text.replace('-','')
                specific_name=parts[1].text
                link=parts[2].find('a')
                if link!=None:
                    
                    name=relative_name.strip()+'-'+specific_name.strip()
                    name=name.replace('\xad','').replace('/','').replace('\xa02','')

                    href=link.get('href')
                    response=rq.get(href)
                    print('    ',response)
                    with open(subfolder+'\\'+name+'.pdf', 'wb') as f:
                        f.write(response.content)
                        
    print('Downloaded document nr '+str(document_number)+':',name)
